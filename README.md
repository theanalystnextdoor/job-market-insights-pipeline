# ğŸ’¼ Job Market Insights Pipeline

This project scrapes job listings from multiple websites, stores them in a MySQL database, and is designed to support future job trend insights and visualizations.

---

## ğŸš€ Tools & Technologies

- **Python** â€“ Web scraping and automation
- **MySQL** â€“ Structured data storage
- **Git & GitHub** â€“ Version control
- **Power BI** â€“ (Optional) Visualization

---

## ğŸ“‚ Project Structure

job-market-insights-pipeline/
â”‚
â”œâ”€â”€ scraper/ # Web scraping scripts (RemoteOK & Remotive)
â”œâ”€â”€ sql/ # SQL schema setup and cleaning
â”œâ”€â”€ etl/ # ETL logic (optional/placeholder)
â”œâ”€â”€ dashboard/ # Power BI reports (optional)
â”œâ”€â”€ requirements.txt # Python package requirements
â””â”€â”€ README.md # Project documentation


---

## ğŸŒ Data Sources

- [RemoteOK API](https://remoteok.com/api)
- [Remotive API](https://remotive.io/api/remote-jobs)

âŒ Note: WeWorkRemotely could not be scraped due to Cloudflare protections.

---

## âš ï¸ Limitations

- The project currently scrapes only one day's worth of job data.
- Further automation (e.g., scheduled scraping) will improve data quality and insight depth.

---

## ğŸ§  Project Goals

- Practice data engineering principles (web scraping â†’ storing â†’ transforming)
- Explore SQL for data cleaning and analysis
- Showcase technical proficiency in building end-to-end data pipelines

---

---

## ğŸ§  Behind the Logic

### 1. ğŸ” Web Scraping Logic

Each scraping script (`remoteok_scraper.py` and `remotive_scraper.py`) follows a clean and modular approach:

- **HTTP Requests**: Uses `requests` with custom headers to simulate a browser and avoid being blocked.
- **JSON Parsing**: Parses the API response directly, which is cleaner and more stable than scraping raw HTML.
- **Error Handling**: Basic try-except blocks (planned for improvement).
- **Data Extraction**: Extracts relevant fields like job title, company, location, tags, and posted date.

### 2. ğŸ›¢ï¸ MySQL Storage

- **Schema Design**: A single `jobs` table was created to store the scraped data consistently.
- **Constraints**: Column size limits were optimized after discovering overly long values (e.g., `location` column).
- **Connection**: Pythonâ€™s `mysql.connector` library is used to insert records into the MySQL database.
- **Deduplication**: Basic checks (e.g., on job title + company) can be added in the future to avoid duplicates.

### 3. ğŸ” ETL Flow

Though simple, this project follows a basic ETL (Extract, Transform, Load) model:

- **Extract**: Pull data from RemoteOK and Remotive APIs.
- **Transform**: Clean fields (e.g., combine tags, format dates).
- **Load**: Insert into a structured MySQL database.

Future improvements could include:
- Scheduled jobs (via cron or Airflow)
- Staging tables for raw vs. cleaned data
- Detailed logging and retries

## ğŸ› ï¸ How to Run

1. **Clone the repository**
```bash
git clone https://github.com/theanalystnextdoor/job-market-insights-pipeline.git
Install dependencies

bash

pip install -r requirements.txt
Run scraping scripts

bash

python scraper/remoteok_scraper.py
python scraper/remotive_scraper.py

View & analyze data

Load the database in MySQL Workbench

Connect to Power BI for future dashboarding (optional)

ğŸ“ˆ Next Steps
Schedule daily/weekly scraping to collect historical job market data

Perform advanced text cleaning or NLP on job titles/descriptions

Deploy dashboards publicly (Power BI Service or Streamlit)

ğŸ‘¨â€ğŸ’» Author
Kolawole Olaitan
Data Analyst | Portfolio Project
ğŸ”— LinkedIn
